{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6ec4c1a7b76a435baccdcef020471909": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1d96c01e96a0485ab727785a34ecef3e",
              "IPY_MODEL_2be75450379a47a68621f46905f983c9",
              "IPY_MODEL_d1ba88b6db594e6988447dc8386afca8"
            ],
            "layout": "IPY_MODEL_212096645c26474ead386325dcabe667"
          }
        },
        "1d96c01e96a0485ab727785a34ecef3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93bfbe99943b40d2bf969a7ea8dee8fd",
            "placeholder": "​",
            "style": "IPY_MODEL_7f6fa51e9f2144c7b577f7ab0c898389",
            "value": "Loading weights: 100%"
          }
        },
        "2be75450379a47a68621f46905f983c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77b4b95f4bbe49f3ad023f3f5776f322",
            "max": 199,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0426b9ec92ee41119c40d407659630fe",
            "value": 199
          }
        },
        "d1ba88b6db594e6988447dc8386afca8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_797432878eb64d7781ad13108ab2ac49",
            "placeholder": "​",
            "style": "IPY_MODEL_f6cbfa3a4a0b43779fb4a8fb84a276c6",
            "value": " 199/199 [00:00&lt;00:00, 325.99it/s, Materializing param=pooler.dense.weight]"
          }
        },
        "212096645c26474ead386325dcabe667": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93bfbe99943b40d2bf969a7ea8dee8fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f6fa51e9f2144c7b577f7ab0c898389": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77b4b95f4bbe49f3ad023f3f5776f322": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0426b9ec92ee41119c40d407659630fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "797432878eb64d7781ad13108ab2ac49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6cbfa3a4a0b43779fb4a8fb84a276c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db01592f9e824fde83b6a2d09dbe9f20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0ae0201e64cb4ac79d69bed66151d1cd",
              "IPY_MODEL_368a7203475b4886ab88a2dedc99c6d4",
              "IPY_MODEL_1701df8d595747f38f8a2fbb7044d22f"
            ],
            "layout": "IPY_MODEL_ca1a9e2028a546df8aba6fd8a3e34ca2"
          }
        },
        "0ae0201e64cb4ac79d69bed66151d1cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c54b6ba7279449f9977840f0e3d001cb",
            "placeholder": "​",
            "style": "IPY_MODEL_2251fc8d1ebe4c6fb1c8508aa01aafea",
            "value": "Loading weights: 100%"
          }
        },
        "368a7203475b4886ab88a2dedc99c6d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_610a224046e24048a78e763f46f76e37",
            "max": 148,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bf247859160a4ad591325024923a7e52",
            "value": 148
          }
        },
        "1701df8d595747f38f8a2fbb7044d22f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a861e68ea15e45068bf81ae86e6e8380",
            "placeholder": "​",
            "style": "IPY_MODEL_70aa2f46961a4a5e94e085f16a748f49",
            "value": " 148/148 [00:00&lt;00:00, 322.34it/s, Materializing param=transformer.wte.weight]"
          }
        },
        "ca1a9e2028a546df8aba6fd8a3e34ca2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c54b6ba7279449f9977840f0e3d001cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2251fc8d1ebe4c6fb1c8508aa01aafea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "610a224046e24048a78e763f46f76e37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf247859160a4ad591325024923a7e52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a861e68ea15e45068bf81ae86e6e8380": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70aa2f46961a4a5e94e085f16a748f49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b014405bc37b44acb146f4110ae04ae1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dc85f6c3805e479f8b43e69a9b860d9e",
              "IPY_MODEL_ce2e333a3a61403d8f2803b18a2c1d05",
              "IPY_MODEL_cd58cffd93044e91a5f55aa14600fca0"
            ],
            "layout": "IPY_MODEL_8d116d7e77c24a22865c064d942c8668"
          }
        },
        "dc85f6c3805e479f8b43e69a9b860d9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12283ef649ff414eb032d69e955cc9b5",
            "placeholder": "​",
            "style": "IPY_MODEL_d6a78370e0fe460584bee54c7c7eb6a9",
            "value": "Writing model shards: 100%"
          }
        },
        "ce2e333a3a61403d8f2803b18a2c1d05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b844a767f9044bceb4e9a21d71f92deb",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_078a95f4d8524bc8893ecfe14d5b0719",
            "value": 1
          }
        },
        "cd58cffd93044e91a5f55aa14600fca0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e784b12cc064252a77a81a2adba8bd2",
            "placeholder": "​",
            "style": "IPY_MODEL_a8559cd32bd545bcae8da3a13141f0f1",
            "value": " 1/1 [00:16&lt;00:00, 16.58s/it]"
          }
        },
        "8d116d7e77c24a22865c064d942c8668": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12283ef649ff414eb032d69e955cc9b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6a78370e0fe460584bee54c7c7eb6a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b844a767f9044bceb4e9a21d71f92deb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "078a95f4d8524bc8893ecfe14d5b0719": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2e784b12cc064252a77a81a2adba8bd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8559cd32bd545bcae8da3a13141f0f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project 3: Large Language Models for Dialogue Summarization\n",
        "\n",
        "This notebook presents an end-to-end transformer-based approach to abstractive dialogue summarization using the SAMSum dataset. We explore model training, evaluation, and downstream applications.\n",
        "\n",
        "We focus on:\n",
        "- building and fine-tuning a **BERT–GPT-2 encoder–decoder** model for abstractive summarization,\n",
        "- comparing it against a simple **heuristic baseline**,\n",
        "- computing both **custom and official ROUGE metrics**,\n",
        "- performing **qualitative error analysis** with real examples,\n",
        "- designing a **ChatGPT-style action-first prompting layer** on top of the summaries.\n",
        "\n",
        "The notebook is structured as follows:\n",
        "\n",
        "1. Setup and Reproducibility  \n",
        "2. Dataset Loading and Exploratory Data Analysis (EDA)  \n",
        "3. Tokenization and Preprocessing  \n",
        "4. BERT–GPT-2 Encoder–Decoder Model  \n",
        "5. Training with Seq2SeqTrainer  \n",
        "6. Baseline vs. Model: ROUGE Evaluation and Qualitative Analysis  \n",
        "7. Official ROUGE with `evaluate`  \n",
        "8. Action-First ChatGPT Prompting Layer  \n",
        "9. Conclusions and Next Steps\n"
      ],
      "metadata": {
        "id": "h-AHjzquymP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q evaluate rouge-score nltk"
      ],
      "metadata": {
        "id": "bWMOsGZY3x2c"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix Colab environment issues (REQUIRED)\n",
        "!pip install -q --upgrade datasets transformers huggingface_hub evaluate rouge-score"
      ],
      "metadata": {
        "id": "1zIZu0244Mxl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Setup and Reproducibility\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    EncoderDecoderModel,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        ")\n",
        "\n",
        "from evaluate import load as load_metric\n",
        "\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    \"\"\"\n",
        "    Set random seeds for Python, NumPy and PyTorch to improve reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7g4a4XJyyn3e",
        "outputId": "e8bac52f-583e-4b08-f4ba-058e96cb0dbb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Dataset Loading and Quick EDA\n",
        "\n",
        "We use the **SAMSum** dataset, which contains short messenger-style dialogues and\n",
        "human-written abstractive summaries.\n",
        "\n",
        "In this section we:\n",
        "- load the dataset using the `datasets` library,\n",
        "- inspect a few raw examples,\n",
        "- compute quick statistics on dialogue and summary lengths.\n"
      ],
      "metadata": {
        "id": "cxkjCt_nysT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.1 Load the SAMSum dataset\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Try the official SAMSum dataset first\n",
        "try:\n",
        "    samsum = load_dataset(\"samsum\")\n",
        "    print(\"Loaded 'samsum' dataset from the Hub.\")\n",
        "except Exception as e:\n",
        "    print(\"Failed to load 'samsum' directly. Error:\", e)\n",
        "    print(\"Trying the mirror dataset 'knkarthick/samsum' instead...\")\n",
        "    samsum = load_dataset(\"knkarthick/samsum\")\n",
        "    print(\"Loaded 'knkarthick/samsum' dataset successfully.\")\n",
        "\n",
        "print(samsum)\n",
        "print(\"Train size:\", len(samsum[\"train\"]))\n",
        "print(\"Validation size:\", len(samsum[\"validation\"]))\n",
        "print(\"Test size:\", len(samsum[\"test\"]))\n",
        "\n",
        "\n",
        "print(\"Train size:\", len(samsum[\"train\"]))\n",
        "print(\"Validation size:\", len(samsum[\"validation\"]))\n",
        "print(\"Test size:\", len(samsum[\"test\"]))\n",
        "\n",
        "\n",
        "# 2.2 Inspect a few sample dialogues\n",
        "for i in range(2):\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Dialogue:\\n\", samsum[\"train\"][i][\"dialogue\"])\n",
        "    print(\"\\nSummary:\\n\", samsum[\"train\"][i][\"summary\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2cwxgP1yzWa",
        "outputId": "7818d1e1-54b7-4e78-e46b-3c88365f4523"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to load 'samsum' directly. Error: Dataset 'samsum' doesn't exist on the Hub or cannot be accessed.\n",
            "Trying the mirror dataset 'knkarthick/samsum' instead...\n",
            "Loaded 'knkarthick/samsum' dataset successfully.\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'dialogue', 'summary'],\n",
            "        num_rows: 14731\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['id', 'dialogue', 'summary'],\n",
            "        num_rows: 818\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['id', 'dialogue', 'summary'],\n",
            "        num_rows: 819\n",
            "    })\n",
            "})\n",
            "Train size: 14731\n",
            "Validation size: 818\n",
            "Test size: 819\n",
            "Train size: 14731\n",
            "Validation size: 818\n",
            "Test size: 819\n",
            "================================================================================\n",
            "Dialogue:\n",
            " Amanda: I baked  cookies. Do you want some?\n",
            "Jerry: Sure!\n",
            "Amanda: I'll bring you tomorrow :-)\n",
            "\n",
            "Summary:\n",
            " Amanda baked cookies and will bring Jerry some tomorrow.\n",
            "================================================================================\n",
            "Dialogue:\n",
            " Olivia: Who are you voting for in this election? \n",
            "Oliver: Liberals as always.\n",
            "Olivia: Me too!!\n",
            "Oliver: Great\n",
            "\n",
            "Summary:\n",
            " Olivia and Olivier are voting for liberals in this election. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Quick Length Statistics\n",
        "\n",
        "We compute basic statistics (min, max, mean) for character lengths of dialogues and summaries\n",
        "on a subset of the training data. This helps us choose reasonable maximum sequence lengths.\n"
      ],
      "metadata": {
        "id": "P34_6U63zOv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def quick_stats(dataset, n_samples: int = 2000):\n",
        "    \"\"\"\n",
        "    Compute simple statistics (min, max, mean character length)\n",
        "    for dialogues and summaries in a dataset split.\n",
        "    \"\"\"\n",
        "    n_samples = min(n_samples, len(dataset))\n",
        "    dlg_lengths = []\n",
        "    sum_lengths = []\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        dlg = dataset[i][\"dialogue\"]\n",
        "        summ = dataset[i][\"summary\"]\n",
        "        dlg_lengths.append(len(dlg))\n",
        "        sum_lengths.append(len(summ))\n",
        "\n",
        "    stats = {\n",
        "        \"dialogue_min_len\": int(np.min(dlg_lengths)),\n",
        "        \"dialogue_max_len\": int(np.max(dlg_lengths)),\n",
        "        \"dialogue_mean_len\": float(np.mean(dlg_lengths)),\n",
        "        \"summary_min_len\": int(np.min(sum_lengths)),\n",
        "        \"summary_max_len\": int(np.max(sum_lengths)),\n",
        "        \"summary_mean_len\": float(np.mean(sum_lengths)),\n",
        "    }\n",
        "    return stats\n",
        "\n",
        "\n",
        "train_stats = quick_stats(samsum[\"train\"], n_samples=2000)\n",
        "train_stats\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghByApVI1bAO",
        "outputId": "51133f84-945b-471b-a290-71e37fbcae97"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dialogue_min_len': 42,\n",
              " 'dialogue_max_len': 2466,\n",
              " 'dialogue_mean_len': 510.2215,\n",
              " 'summary_min_len': 4,\n",
              " 'summary_max_len': 300,\n",
              " 'summary_mean_len': 111.501}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Tokenization and Preprocessing\n",
        "\n",
        "Our summarization model uses:\n",
        "\n",
        "- **BERT** as the encoder (for the input dialogue),\n",
        "- **GPT-2** as the decoder (for the target summary).\n",
        "\n",
        "We therefore use two tokenizers:\n",
        "\n",
        "- `bert-base-uncased` for dialogues,\n",
        "- `gpt2` for summaries.\n",
        "\n",
        "Preprocessing steps:\n",
        "- truncate/pad dialogues and summaries to fixed maximum lengths,\n",
        "- tokenize with the corresponding tokenizers,\n",
        "- replace padding tokens in labels with `-100` so that they are ignored by the loss function.\n"
      ],
      "metadata": {
        "id": "UmZFl6VP2Q8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.1 Load tokenizers for encoder and decoder\n",
        "\n",
        "encoder_name = \"bert-base-uncased\"\n",
        "decoder_name = \"gpt2\"\n",
        "\n",
        "enc_tok = AutoTokenizer.from_pretrained(encoder_name, use_fast=True)\n",
        "dec_tok = AutoTokenizer.from_pretrained(decoder_name, use_fast=True)\n",
        "\n",
        "# GPT-2 does not have a PAD token by default, so we add one\n",
        "if dec_tok.pad_token is None:\n",
        "    dec_tok.add_special_tokens({\"pad_token\": dec_tok.eos_token})\n",
        "\n",
        "print(\"Encoder vocab size:\", enc_tok.vocab_size)\n",
        "print(\"Decoder vocab size:\", len(dec_tok))\n",
        "print(\"Decoder PAD token:\", dec_tok.pad_token, dec_tok.pad_token_id)\n",
        "\n",
        "# 3.2 Define maximum sequence lengths based on data stats\n",
        "MAX_INPUT_LEN = 256   # maximum tokens for dialogues\n",
        "MAX_TARGET_LEN = 64   # maximum tokens for summaries\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uwqnpRi2X9T",
        "outputId": "a956d0f5-80c9-453a-b65f-c747a59dbdf6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder vocab size: 30522\n",
            "Decoder vocab size: 50257\n",
            "Decoder PAD token: <|endoftext|> 50256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.3 Preprocessing Function\n",
        "\n",
        "def preprocess_batch(batch):\n",
        "    \"\"\"\n",
        "    Tokenize dialogues and summaries for encoder-decoder training.\n",
        "\n",
        "    - Encoder: BERT tokenizer on 'dialogue'\n",
        "    - Decoder: GPT-2 tokenizer on 'summary'\n",
        "    - PAD tokens in labels are replaced with -100 so they are ignored by the loss.\n",
        "\n",
        "    Args:\n",
        "        batch: A batch of examples with 'dialogue' and 'summary' fields.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary with input_ids, attention_mask, and labels.\n",
        "    \"\"\"\n",
        "    # Tokenize dialogues for the encoder\n",
        "    enc = enc_tok(\n",
        "        batch[\"dialogue\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=MAX_INPUT_LEN,\n",
        "    )\n",
        "\n",
        "    # Tokenize summaries for the decoder\n",
        "    dec = dec_tok(\n",
        "        batch[\"summary\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=MAX_TARGET_LEN,\n",
        "    )\n",
        "\n",
        "    # Use decoder token IDs as labels\n",
        "    enc[\"labels\"] = dec[\"input_ids\"]\n",
        "\n",
        "    # Replace padding token IDs in labels with -100 so they are ignored by the loss\n",
        "    enc[\"labels\"] = [\n",
        "        [(t if t != dec_tok.pad_token_id else -100) for t in seq]\n",
        "        for seq in enc[\"labels\"]\n",
        "    ]\n",
        "\n",
        "    return enc\n",
        "\n",
        "\n",
        "# 3.4 (Optional) Subsample dataset for faster training (Colab-friendly)\n",
        "\n",
        "TRAIN_N = 6000\n",
        "VAL_N = 1500\n",
        "\n",
        "train_raw = samsum[\"train\"].select(range(min(TRAIN_N, len(samsum[\"train\"]))))\n",
        "val_raw = samsum[\"validation\"].select(range(min(VAL_N, len(samsum[\"validation\"]))))\n",
        "\n",
        "# 3.5 Apply preprocessing\n",
        "\n",
        "train_tokenized = train_raw.map(\n",
        "    preprocess_batch,\n",
        "    batched=True,\n",
        "    remove_columns=[\"dialogue\", \"summary\"],\n",
        ")\n",
        "\n",
        "val_tokenized = val_raw.map(\n",
        "    preprocess_batch,\n",
        "    batched=True,\n",
        "    remove_columns=[\"dialogue\", \"summary\"],\n",
        ")\n",
        "\n",
        "train_tokenized[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sq6pnY5C12A",
        "outputId": "b8710048-34a1-409e-ca03-f6bdf240cb5e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '13818513',\n",
              " 'input_ids': [101,\n",
              "  8282,\n",
              "  1024,\n",
              "  1045,\n",
              "  17776,\n",
              "  16324,\n",
              "  1012,\n",
              "  2079,\n",
              "  2017,\n",
              "  2215,\n",
              "  2070,\n",
              "  1029,\n",
              "  6128,\n",
              "  1024,\n",
              "  2469,\n",
              "  999,\n",
              "  8282,\n",
              "  1024,\n",
              "  1045,\n",
              "  1005,\n",
              "  2222,\n",
              "  3288,\n",
              "  2017,\n",
              "  4826,\n",
              "  1024,\n",
              "  1011,\n",
              "  1007,\n",
              "  102,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " 'token_type_ids': [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " 'attention_mask': [1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " 'labels': [5840,\n",
              "  5282,\n",
              "  22979,\n",
              "  14746,\n",
              "  290,\n",
              "  481,\n",
              "  2222,\n",
              "  13075,\n",
              "  617,\n",
              "  9439,\n",
              "  13,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100]}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. BERT–GPT-2 Encoder–Decoder Model\n",
        "\n",
        "We build a **BERT–GPT-2 encoder–decoder model** initialized from pretrained weights:\n",
        "\n",
        "- the encoder is `bert-base-uncased`,\n",
        "- the decoder is `gpt2`,\n",
        "- cross-attention is enabled in the decoder by default,\n",
        "- we configure the correct special tokens (PAD, EOS, decoder start).\n",
        "\n",
        "This setup leverages strong contextual representations from BERT\n",
        "and fluent generative capabilities from GPT-2, making it suitable for abstractive dialogue summarization.\n"
      ],
      "metadata": {
        "id": "2i_9aJ5U2k7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.1 Build encoder–decoder model from pretrained components\n",
        "\n",
        "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
        "    encoder_name,\n",
        "    decoder_name,\n",
        ")\n",
        "\n",
        "# If we added a PAD token to the decoder tokenizer, we need to resize embeddings\n",
        "model.decoder.resize_token_embeddings(len(dec_tok))\n",
        "\n",
        "# 4.2 Configure special tokens and generation settings\n",
        "# Special tokens (это правильно оставляем в model.config)\n",
        "model.config.decoder_start_token_id = dec_tok.pad_token_id\n",
        "model.config.eos_token_id = dec_tok.eos_token_id\n",
        "model.config.pad_token_id = dec_tok.pad_token_id\n",
        "model.config.vocab_size = model.config.decoder.vocab_size\n",
        "\n",
        "# >>> Generation parameters должны жить в model.generation_config <<<\n",
        "gen_cfg = model.generation_config\n",
        "gen_cfg.max_length = MAX_TARGET_LEN\n",
        "gen_cfg.min_length = 8\n",
        "gen_cfg.no_repeat_ngram_size = 2\n",
        "#gen_cfg.early_stopping = True\n",
        "model.generation_config = gen_cfg\n",
        "\n",
        "\n",
        "model.to(device)\n",
        "print(\"Model initialized and moved to device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591,
          "referenced_widgets": [
            "6ec4c1a7b76a435baccdcef020471909",
            "1d96c01e96a0485ab727785a34ecef3e",
            "2be75450379a47a68621f46905f983c9",
            "d1ba88b6db594e6988447dc8386afca8",
            "212096645c26474ead386325dcabe667",
            "93bfbe99943b40d2bf969a7ea8dee8fd",
            "7f6fa51e9f2144c7b577f7ab0c898389",
            "77b4b95f4bbe49f3ad023f3f5776f322",
            "0426b9ec92ee41119c40d407659630fe",
            "797432878eb64d7781ad13108ab2ac49",
            "f6cbfa3a4a0b43779fb4a8fb84a276c6",
            "db01592f9e824fde83b6a2d09dbe9f20",
            "0ae0201e64cb4ac79d69bed66151d1cd",
            "368a7203475b4886ab88a2dedc99c6d4",
            "1701df8d595747f38f8a2fbb7044d22f",
            "ca1a9e2028a546df8aba6fd8a3e34ca2",
            "c54b6ba7279449f9977840f0e3d001cb",
            "2251fc8d1ebe4c6fb1c8508aa01aafea",
            "610a224046e24048a78e763f46f76e37",
            "bf247859160a4ad591325024923a7e52",
            "a861e68ea15e45068bf81ae86e6e8380",
            "70aa2f46961a4a5e94e085f16a748f49"
          ]
        },
        "id": "XXmWrl6k2mM9",
        "outputId": "26255018-b8d8-4dff-f785-feaa65e95127"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ec4c1a7b76a435baccdcef020471909"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BertModel LOAD REPORT from: bert-base-uncased\n",
            "Key                                        | Status     |  | \n",
            "-------------------------------------------+------------+--+-\n",
            "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
            "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
            "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
            "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
            "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
            "cls.predictions.bias                       | UNEXPECTED |  | \n",
            "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/148 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db01592f9e824fde83b6a2d09dbe9f20"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GPT2LMHeadModel LOAD REPORT from: gpt2\n",
            "Key                                                 | Status     | \n",
            "----------------------------------------------------+------------+-\n",
            "h.{0...11}.attn.bias                                | UNEXPECTED | \n",
            "transformer.h.{0...11}.ln_cross_attn.weight         | MISSING    | \n",
            "transformer.h.{0...11}.crossattention.c_proj.weight | MISSING    | \n",
            "transformer.h.{0...11}.ln_cross_attn.bias           | MISSING    | \n",
            "transformer.h.{0...11}.crossattention.c_proj.bias   | MISSING    | \n",
            "transformer.h.{0...11}.crossattention.c_attn.bias   | MISSING    | \n",
            "transformer.h.{0...11}.crossattention.c_attn.weight | MISSING    | \n",
            "transformer.h.{0...11}.crossattention.q_attn.weight | MISSING    | \n",
            "transformer.h.{0...11}.crossattention.q_attn.bias   | MISSING    | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
            "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model initialized and moved to device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Data Collator and Training Setup\n",
        "\n",
        "We use the Hugging Face **Seq2SeqTrainer** API:\n",
        "\n",
        "- `DataCollatorForSeq2Seq` dynamically pads examples to the longest sequence in the batch,\n",
        "- `Seq2SeqTrainingArguments` define hyperparameters,\n",
        "- `Seq2SeqTrainer` handles training, evaluation, and generation.\n",
        "\n",
        "We keep training settings moderate to fit within typical Google Colab limits,\n",
        "but they can be scaled up for more serious experiments.\n"
      ],
      "metadata": {
        "id": "EfXP2GwE2qtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.1 Data collator for seq2seq models\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=enc_tok,\n",
        "    model=model,\n",
        "    padding=\"longest\",\n",
        ")\n",
        "\n",
        "\n",
        "# 5.2 Seq2Seq training arguments (version-safe)\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"outputs_samsum_bert2gpt2\",\n",
        "    num_train_epochs=1,\n",
        "    max_steps=250,                      # keep small for Colab; increase if you can\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    predict_with_generate=True,\n",
        "    fp16=False,                         # set True if your GPU supports it\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    logging_steps=50,                   # how often to log\n",
        "    save_steps=1000,                    # effectively no checkpoints\n",
        "    save_total_limit=1,\n",
        "    report_to=[],                       # no external logging (W&B etc.)\n",
        ")\n",
        "\n",
        "# 5.3 Initialize the Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized,\n",
        "    eval_dataset=val_tokenized,\n",
        "    data_collator=data_collator,\n",
        ")\n"
      ],
      "metadata": {
        "id": "JayA7Inx2tqn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Training the Encoder–Decoder Model\n",
        "\n",
        "We now fine-tune the pretrained BERT–GPT-2 encoder–decoder model on SAMSum.\n",
        "\n",
        "The `Seq2SeqTrainer`:\n",
        "- performs forward and backward passes,\n",
        "- handles optimizer and learning rate scheduling,\n",
        "- periodically evaluates on the validation set.\n",
        "\n",
        "For a stronger model, we would train for more steps and possibly with larger batch sizes.\n",
        "Here we focus on a **Colab-friendly but realistic** training run.\n"
      ],
      "metadata": {
        "id": "VeE8zds-2v4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.1 Train the model (this may take several minutes depending on hardware)\n",
        "\n",
        "train_result = trainer.train()\n",
        "print(\"Training finished. Global step:\", train_result.global_step)\n",
        "\n",
        "# 6.2 Evaluate the model using Trainer's built-in evaluation\n",
        "eval_metrics = trainer.evaluate()\n",
        "print(\"Validation metrics from Trainer:\", eval_metrics)\n"
      ],
      "metadata": {
        "id": "9IOYPxXE2yvn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472,
          "referenced_widgets": [
            "b014405bc37b44acb146f4110ae04ae1",
            "dc85f6c3805e479f8b43e69a9b860d9e",
            "ce2e333a3a61403d8f2803b18a2c1d05",
            "cd58cffd93044e91a5f55aa14600fca0",
            "8d116d7e77c24a22865c064d942c8668",
            "12283ef649ff414eb032d69e955cc9b5",
            "d6a78370e0fe460584bee54c7c7eb6a9",
            "b844a767f9044bceb4e9a21d71f92deb",
            "078a95f4d8524bc8893ecfe14d5b0719",
            "2e784b12cc064252a77a81a2adba8bd2",
            "a8559cd32bd545bcae8da3a13141f0f1"
          ]
        },
        "outputId": "df99d033-8fef-49be-a0e9-0f7da6c63c65"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:445: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 31:00, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>4.319880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>4.016059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>3.903568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>3.967823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>3.886154</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b014405bc37b44acb146f4110ae04ae1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training finished. Global step: 250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:445: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='409' max='409' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [409/409 13:48]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation metrics from Trainer: {'eval_loss': 3.78239369392395, 'eval_runtime': 830.1496, 'eval_samples_per_second': 0.985, 'eval_steps_per_second': 0.493, 'epoch': 0.08333333333333333}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Baseline and Custom ROUGE Implementation\n",
        "\n",
        "To better interpret model performance, we define:\n",
        "\n",
        "1. A very simple **baseline summarizer**:\n",
        "   - It takes the first and last sentence of the dialogue as the \"summary\".\n",
        "2. A custom implementation of **ROUGE-1** and **ROUGE-L** F1:\n",
        "   - This demonstrates understanding of how ROUGE works,\n",
        "   - and allows us to compare baseline vs. model on the same metric.\n"
      ],
      "metadata": {
        "id": "7uGEdGoB20nZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_into_sentences(text: str):\n",
        "    \"\"\"\n",
        "    Very simple sentence splitter based on punctuation.\n",
        "    This is intentionally lightweight and heuristic.\n",
        "    \"\"\"\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
        "    sentences = [s for s in sentences if s]\n",
        "    return sentences\n",
        "\n",
        "\n",
        "def baseline_head_tail(dialogue: str, max_sentences: int = 2) -> str:\n",
        "    \"\"\"\n",
        "    Baseline summarizer: take the first and last sentence from the dialogue.\n",
        "    If the dialogue is very short, return what is available.\n",
        "\n",
        "    This provides a naive extractive baseline to compare against our model.\n",
        "    \"\"\"\n",
        "    sentences = split_into_sentences(dialogue)\n",
        "    if not sentences:\n",
        "        return \"\"\n",
        "    if len(sentences) <= max_sentences:\n",
        "        return \" \".join(sentences)\n",
        "    # use the first and last sentence\n",
        "    return sentences[0] + \" \" + sentences[-1]\n"
      ],
      "metadata": {
        "id": "a6ejIfFW22VS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "\n",
        "def _tok(s: str):\n",
        "    \"\"\"\n",
        "    Very simple whitespace-based tokenizer with lowercasing.\n",
        "    \"\"\"\n",
        "    return s.lower().split()\n",
        "\n",
        "\n",
        "def rouge1_f1_single(pred: str, ref: str) -> float:\n",
        "    \"\"\"\n",
        "    Compute ROUGE-1 F1 score between a single prediction and reference.\n",
        "    \"\"\"\n",
        "    pred_tokens = _tok(pred)\n",
        "    ref_tokens = _tok(ref)\n",
        "    if not pred_tokens or not ref_tokens:\n",
        "        return 0.0\n",
        "\n",
        "    pred_counts = Counter(pred_tokens)\n",
        "    ref_counts = Counter(ref_tokens)\n",
        "\n",
        "    overlap = sum((pred_counts & ref_counts).values())\n",
        "    if overlap == 0:\n",
        "        return 0.0\n",
        "\n",
        "    precision = overlap / len(pred_tokens)\n",
        "    recall = overlap / len(ref_tokens)\n",
        "    if precision + recall == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return 2 * precision * recall / (precision + recall)\n",
        "\n",
        "\n",
        "def lcs_len(x: list, y: list) -> int:\n",
        "    \"\"\"\n",
        "    Longest Common Subsequence (LCS) length for ROUGE-L.\n",
        "    \"\"\"\n",
        "    m, n = len(x), len(y)\n",
        "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "    for i in range(m):\n",
        "        for j in range(n):\n",
        "            if x[i] == y[j]:\n",
        "                dp[i + 1][j + 1] = dp[i][j] + 1\n",
        "            else:\n",
        "                dp[i + 1][j + 1] = max(dp[i][j + 1], dp[i + 1][j])\n",
        "    return dp[m][n]\n",
        "\n",
        "\n",
        "def rougeL_f1_single(pred: str, ref: str) -> float:\n",
        "    \"\"\"\n",
        "    Compute ROUGE-L F1 score between a single prediction and reference.\n",
        "    \"\"\"\n",
        "    pred_tokens = _tok(pred)\n",
        "    ref_tokens = _tok(ref)\n",
        "    if not pred_tokens or not ref_tokens:\n",
        "        return 0.0\n",
        "\n",
        "    lcs = lcs_len(pred_tokens, ref_tokens)\n",
        "    if lcs == 0:\n",
        "        return 0.0\n",
        "\n",
        "    precision = lcs / len(pred_tokens)\n",
        "    recall = lcs / len(ref_tokens)\n",
        "    if precision + recall == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return 2 * precision * recall / (precision + recall)\n",
        "\n",
        "\n",
        "def avg_scores(pred_list, ref_list, metric_fn):\n",
        "    \"\"\"\n",
        "    Compute the average of a given ROUGE metric over a list of predictions.\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "    for p, r in zip(pred_list, ref_list):\n",
        "        scores.append(metric_fn(p, r))\n",
        "    return float(np.mean(scores)), scores\n"
      ],
      "metadata": {
        "id": "pWQSdRz2253F"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Generating Model Summaries & Comparing with Baseline\n",
        "\n",
        "In this section we:\n",
        "\n",
        "1. Build a small evaluation subset from the validation split.\n",
        "2. Generate summaries using:\n",
        "   - the **baseline** heuristic,\n",
        "   - our fine-tuned **encoder–decoder model**.\n",
        "3. Compute **custom ROUGE-1 and ROUGE-L** scores for both.\n",
        "4. Inspect a few **qualitative examples** to understand typical model behavior.\n"
      ],
      "metadata": {
        "id": "G3LD8CpK28Ee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(dialogue: str, max_new_tokens: int = 48) -> str:\n",
        "    \"\"\"\n",
        "    Generate an abstractive summary for a single dialogue using the\n",
        "    encoder–decoder model.\n",
        "\n",
        "    Args:\n",
        "        dialogue: Original SAMSum-style conversation (multi-turn).\n",
        "        max_new_tokens: Maximum number of tokens to generate.\n",
        "\n",
        "    Returns:\n",
        "        Decoded summary string.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    inputs = enc_tok(\n",
        "        [dialogue],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=MAX_INPUT_LEN,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            num_beams=4,\n",
        "            early_stopping=True,\n",
        "        )\n",
        "\n",
        "    summary = dec_tok.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    return summary.strip()\n"
      ],
      "metadata": {
        "id": "qrZTcbpx2-Ol"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8.2 Build a small evaluation subset for qualitative and quantitative analysis\n",
        "EVAL_N = 150\n",
        "eval_raw = val_raw.select(range(min(EVAL_N, len(val_raw))))\n",
        "\n",
        "dialogues = [ex[\"dialogue\"] for ex in eval_raw]\n",
        "references = [ex[\"summary\"] for ex in eval_raw]\n",
        "\n",
        "# Baseline predictions\n",
        "baseline_preds = [baseline_head_tail(dlg) for dlg in dialogues]\n",
        "\n",
        "# Model predictions\n",
        "model_preds = [generate_summary(dlg) for dlg in dialogues]\n",
        "\n",
        "print(\"Example model prediction:\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Dialogue:\\n\", dialogues[0])\n",
        "print(\"\\nReference summary:\\n\", references[0])\n",
        "print(\"\\nBaseline summary:\\n\", baseline_preds[0])\n",
        "print(\"\\nModel summary:\\n\", model_preds[0])\n"
      ],
      "metadata": {
        "id": "SS452oZK3AD-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adf7bb60-3007-4b29-8eac-88df04c11f88"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example model prediction:\n",
            "================================================================================\n",
            "Dialogue:\n",
            " A: Hi Tom, are you busy tomorrow’s afternoon?\n",
            "B: I’m pretty sure I am. What’s up?\n",
            "A: Can you go with me to the animal shelter?.\n",
            "B: What do you want to do?\n",
            "A: I want to get a puppy for my son.\n",
            "B: That will make him so happy.\n",
            "A: Yeah, we’ve discussed it many times. I think he’s ready now.\n",
            "B: That’s good. Raising a dog is a tough issue. Like having a baby ;-) \n",
            "A: I'll get him one of those little dogs.\n",
            "B: One that won't grow up too big;-)\n",
            "A: And eat too much;-))\n",
            "B: Do you know which one he would like?\n",
            "A: Oh, yes, I took him there last Monday. He showed me one that he really liked.\n",
            "B: I bet you had to drag him away.\n",
            "A: He wanted to take it home right away ;-).\n",
            "B: I wonder what he'll name it.\n",
            "A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\n",
            "\n",
            "Reference summary:\n",
            " A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
            "\n",
            "Baseline summary:\n",
            " A: Hi Tom, are you busy tomorrow’s afternoon? A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\n",
            "\n",
            "Model summary:\n",
            " Sylvia is going to go to the beach. She will be there for a few hours.  The beach is a bit crowded, so she will have to wait for the bus to take her to a beach in the middle of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8.3 Compute custom ROUGE-1 and ROUGE-L for baseline and model\n",
        "\n",
        "base_r1_mean, base_r1_list = avg_scores(baseline_preds, references, rouge1_f1_single)\n",
        "base_rL_mean, base_rL_list = avg_scores(baseline_preds, references, rougeL_f1_single)\n",
        "\n",
        "mod_r1_mean, mod_r1_list = avg_scores(model_preds, references, rouge1_f1_single)\n",
        "mod_rL_mean, mod_rL_list = avg_scores(model_preds, references, rougeL_f1_single)\n",
        "\n",
        "print(\"=== Custom ROUGE (approximate) ===\")\n",
        "print(f\"Baseline ROUGE-1 F1: {base_r1_mean:.4f}\")\n",
        "print(f\"Baseline ROUGE-L F1: {base_rL_mean:.4f}\")\n",
        "print(f\"Model    ROUGE-1 F1: {mod_r1_mean:.4f}\")\n",
        "print(f\"Model    ROUGE-L F1: {mod_rL_mean:.4f}\")\n"
      ],
      "metadata": {
        "id": "niL2dPoe3Bz7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4405d27f-2e99-4a4d-c95d-69a960f3cb35"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Custom ROUGE (approximate) ===\n",
            "Baseline ROUGE-1 F1: 0.1464\n",
            "Baseline ROUGE-L F1: 0.1281\n",
            "Model    ROUGE-1 F1: 0.1376\n",
            "Model    ROUGE-L F1: 0.1095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8.4 Show a few qualitative examples with scores\n",
        "\n",
        "for i in range(3):\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Example {i+1}\")\n",
        "    print(\"--- Dialogue ---\")\n",
        "    print(dialogues[i])\n",
        "    print(\"\\n--- Reference summary ---\")\n",
        "    print(references[i])\n",
        "    print(\"\\n--- Baseline summary ---\")\n",
        "    print(baseline_preds[i])\n",
        "    print(\"\\n--- Model summary ---\")\n",
        "    print(model_preds[i])\n",
        "    print(\n",
        "        f\"\\nBaseline ROUGE-1 F1: {base_r1_list[i]:.4f} | ROUGE-L F1: {base_rL_list[i]:.4f}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"Model    ROUGE-1 F1: {mod_r1_list[i]:.4f} | ROUGE-L F1: {mod_rL_list[i]:.4f}\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "xvr6egke3E-0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc6c5b1e-2b6c-4136-a57b-27751fea5ace"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Example 1\n",
            "--- Dialogue ---\n",
            "A: Hi Tom, are you busy tomorrow’s afternoon?\n",
            "B: I’m pretty sure I am. What’s up?\n",
            "A: Can you go with me to the animal shelter?.\n",
            "B: What do you want to do?\n",
            "A: I want to get a puppy for my son.\n",
            "B: That will make him so happy.\n",
            "A: Yeah, we’ve discussed it many times. I think he’s ready now.\n",
            "B: That’s good. Raising a dog is a tough issue. Like having a baby ;-) \n",
            "A: I'll get him one of those little dogs.\n",
            "B: One that won't grow up too big;-)\n",
            "A: And eat too much;-))\n",
            "B: Do you know which one he would like?\n",
            "A: Oh, yes, I took him there last Monday. He showed me one that he really liked.\n",
            "B: I bet you had to drag him away.\n",
            "A: He wanted to take it home right away ;-).\n",
            "B: I wonder what he'll name it.\n",
            "A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\n",
            "\n",
            "--- Reference summary ---\n",
            "A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
            "\n",
            "--- Baseline summary ---\n",
            "A: Hi Tom, are you busy tomorrow’s afternoon? A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\n",
            "\n",
            "--- Model summary ---\n",
            "Sylvia is going to go to the beach. She will be there for a few hours.  The beach is a bit crowded, so she will have to wait for the bus to take her to a beach in the middle of\n",
            "\n",
            "Baseline ROUGE-1 F1: 0.0364 | ROUGE-L F1: 0.0364\n",
            "Model    ROUGE-1 F1: 0.3478 | ROUGE-L F1: 0.2029\n",
            "================================================================================\n",
            "Example 2\n",
            "--- Dialogue ---\n",
            "Emma: I’ve just fallen in love with this advent calendar! Awesome! I wanna one for my kids!\n",
            "Rob: I used to get one every year as a child! Loved them! \n",
            "Emma: Yeah, i remember! they were filled with chocolates!\n",
            "Lauren: they are different these days! much more sophisticated! Haha!\n",
            "Rob: yeah, they can be fabric/ wooden, shop bought/ homemade, filled with various stuff\n",
            "Emma: what do you fit inside?\n",
            "Lauren: small toys, Christmas decorations, creative stuff, hair bands & clips, stickers, pencils & rubbers, small puzzles, sweets\n",
            "Emma: WOW! That’s brill! X\n",
            "Lauren: i add one more very special thing as well- little notes asking my children to do something nice for someone else\n",
            "Rob: i like that! My sister adds notes asking her kids questions about christmas such as What did the 3 wise men bring? etc\n",
            "Lauren: i reckon it prepares them for Christmas \n",
            "Emma: and makes it more about traditions and being kind to other people\n",
            "Lauren: my children get very excited every time they get one!\n",
            "Emma: i can see why! :)\n",
            "\n",
            "--- Reference summary ---\n",
            "Emma and Rob love the advent calendar. Lauren fits inside calendar various items, for instance, small toys and Christmas decorations. Her children are excited whenever they get the calendar.\n",
            "\n",
            "--- Baseline summary ---\n",
            "Emma: I’ve just fallen in love with this advent calendar! :)\n",
            "\n",
            "--- Model summary ---\n",
            "Sylvia is going to go to the beach. She will be there for a few hours.  The beach is a bit crowded, so she will have to wait for the bus to take her to a beach in the middle of\n",
            "\n",
            "Baseline ROUGE-1 F1: 0.1000 | ROUGE-L F1: 0.1000\n",
            "Model    ROUGE-1 F1: 0.1143 | ROUGE-L F1: 0.1143\n",
            "================================================================================\n",
            "Example 3\n",
            "--- Dialogue ---\n",
            "Jackie: Madison is pregnant\n",
            "Jackie: but she doesn't wanna talk about it\n",
            "Iggy: why\n",
            "Jackie: I don't know why because she doesn't wanna talk about it\n",
            "Iggy: ok\n",
            "Jackie: I wanted to prepare you for it because people get super excited and ask lots of questions\n",
            "Jackie: and she looked way more anxious than excited\n",
            "Iggy: she's probably worrying about it\n",
            "Iggy: she's taking every commitment really seriously\n",
            "Jackie: it could be money problems or relationship problems\n",
            "Iggy: or maybe she wants an abortion\n",
            "Jackie: it could be all of the above\n",
            "Iggy: but you know what?\n",
            "Iggy: once my friend was pregnant and I couldn't bring myself to be happy about it\n",
            "Jackie: why?\n",
            "Iggy: I felt they were immature and I couldn't picture this couple as parents\n",
            "Jackie: I felt similar way on Patricia's wedding\n",
            "Iggy: Patricia Stevens?\n",
            "Jackie: yes\n",
            "Iggy: so we're talking about the same person\n",
            "Jackie: what a coincidence\n",
            "Jackie: so she's pregnant?\n",
            "Iggy: she thought she was\n",
            "Jackie: damn...\n",
            "\n",
            "--- Reference summary ---\n",
            "Madison is pregnant but she doesn't want to talk about it. Patricia Stevens got married and she thought she was pregnant. \n",
            "\n",
            "--- Baseline summary ---\n",
            "Jackie: Madison is pregnant\n",
            "Jackie: but she doesn't wanna talk about it\n",
            "Iggy: why\n",
            "Jackie: I don't know why because she doesn't wanna talk about it\n",
            "Iggy: ok\n",
            "Jackie: I wanted to prepare you for it because people get super excited and ask lots of questions\n",
            "Jackie: and she looked way more anxious than excited\n",
            "Iggy: she's probably worrying about it\n",
            "Iggy: she's taking every commitment really seriously\n",
            "Jackie: it could be money problems or relationship problems\n",
            "Iggy: or maybe she wants an abortion\n",
            "Jackie: it could be all of the above\n",
            "Iggy: but you know what? Iggy: she thought she was\n",
            "Jackie: damn...\n",
            "\n",
            "--- Model summary ---\n",
            "Sylvia is going to go to the beach. She's not sure if it's a good idea to stay in the middle of the night.  The beach is too cold for her and she'll have to sleep in a tent\n",
            "\n",
            "Baseline ROUGE-1 F1: 0.2240 | ROUGE-L F1: 0.2080\n",
            "Model    ROUGE-1 F1: 0.1000 | ROUGE-L F1: 0.1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Official ROUGE via `evaluate`\n",
        "\n",
        "In addition to the custom implementation, we compute **standard ROUGE metrics**\n",
        "using the `evaluate` library. This provides:\n",
        "\n",
        "- ROUGE-1, ROUGE-2, and ROUGE-L,\n",
        "- precision, recall, and F1 scores.\n",
        "\n",
        "We report these for our encoder–decoder model on the evaluation subset.\n"
      ],
      "metadata": {
        "id": "rFU7P9C53Gq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load as load_metric\n",
        "\n",
        "rouge_metric = load_metric(\"rouge\")\n",
        "\n",
        "rouge_result = rouge_metric.compute(\n",
        "    predictions=model_preds,\n",
        "    references=references,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "print(\"=== HuggingFace evaluate ROUGE (model) ===\")\n",
        "for key in [\"rouge1\", \"rouge2\", \"rougeL\"]:\n",
        "    score = rouge_result[key]\n",
        "    score_float = float(score)\n",
        "    print(f\"{key.upper()} F1: {score_float:.4f}\")\n"
      ],
      "metadata": {
        "id": "n8t2VpRK3JC-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "325bb1be-9dfc-4af9-85b6-2aacdd63c9d2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== HuggingFace evaluate ROUGE (model) ===\n",
            "ROUGE1 F1: 0.1443\n",
            "ROUGE2 F1: 0.0153\n",
            "ROUGEL F1: 0.1127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Action-First ChatGPT Prompting Layer\n",
        "\n",
        "Beyond pure summarization, we add an **action-first prompting layer** designed for ChatGPT\n",
        "(or any comparable auto-regressive LLM).\n",
        "\n",
        "Workflow:\n",
        "\n",
        "1. Our encoder–decoder model produces a concise summary of a conversation.\n",
        "2. We wrap this summary into a structured prompt that asks the LLM to:\n",
        "   - extract key decisions,\n",
        "   - list action items with owners and deadlines,\n",
        "   - identify open questions or unresolved issues.\n",
        "\n",
        "This demonstrates how a custom summarization model can be integrated into a larger LLM-powered assistant for productivity and decision support.\n",
        "\n",
        "This experiment further highlights how summarization errors can propagate into downstream LLM-based systems, emphasizing the importance of faithful and grounded summaries in real-world applications.\n"
      ],
      "metadata": {
        "id": "kMVQ9x1t3LHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ACTION_DIGEST_PROMPT = \"\"\"\n",
        "You are an AI assistant that helps busy professionals keep track of decisions and next steps.\n",
        "\n",
        "Below is a summary of a conversation. Your job is:\n",
        "1. Extract the key decisions that were made.\n",
        "2. Extract the action items with clear owners and deadlines if mentioned.\n",
        "3. Highlight any open questions or unresolved issues.\n",
        "\n",
        "Return your output in the following structured format:\n",
        "\n",
        "Decisions:\n",
        "- ...\n",
        "\n",
        "Action Items:\n",
        "- [Owner] ... (deadline: ...)\n",
        "\n",
        "Open Questions:\n",
        "- ...\n",
        "\n",
        "Conversation summary:\n",
        "\\\"\\\"\\\"{summary}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def build_action_digest_prompt(summary: str) -> str:\n",
        "    \"\"\"\n",
        "    Wrap a model-generated summary into an 'action-first' prompt that can be\n",
        "    sent to ChatGPT (or another LLM) to extract decisions and next steps.\n",
        "    \"\"\"\n",
        "    return ACTION_DIGEST_PROMPT.format(summary=summary)\n",
        "\n",
        "\n",
        "# 10.2 Example: use a model-generated summary and build the LLM prompt\n",
        "example_idx = 0\n",
        "example_summary = model_preds[example_idx]\n",
        "example_prompt = build_action_digest_prompt(example_summary)\n",
        "\n",
        "print(\"=== Example action-first prompt for ChatGPT ===\")\n",
        "print(example_prompt)\n"
      ],
      "metadata": {
        "id": "orJAhtAl3Mvs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ab0d95f-c3bd-4314-f8b8-ff9fadf4a176"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Example action-first prompt for ChatGPT ===\n",
            "\n",
            "You are an AI assistant that helps busy professionals keep track of decisions and next steps.\n",
            "\n",
            "Below is a summary of a conversation. Your job is:\n",
            "1. Extract the key decisions that were made.\n",
            "2. Extract the action items with clear owners and deadlines if mentioned.\n",
            "3. Highlight any open questions or unresolved issues.\n",
            "\n",
            "Return your output in the following structured format:\n",
            "\n",
            "Decisions:\n",
            "- ...\n",
            "\n",
            "Action Items:\n",
            "- [Owner] ... (deadline: ...)\n",
            "\n",
            "Open Questions:\n",
            "- ...\n",
            "\n",
            "Conversation summary:\n",
            "\"\"\"Sylvia is going to go to the beach. She will be there for a few hours.  The beach is a bit crowded, so she will have to wait for the bus to take her to a beach in the middle of\"\"\"\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Conclusions and Next Steps\n",
        "\n",
        "In this project, we developed and evaluated an end-to-end transformer-based system for abstractive dialogue summarization. We implemented a BERT–GPT-2 encoder–decoder architecture and fine-tuned it on the SAMSum dataset using a Colab-compatible training setup. To establish a meaningful point of comparison, we defined a simple extractive baseline that concatenates the first and last utterances of each dialogue.\n",
        "\n",
        "Model performance was evaluated using both custom-implemented ROUGE-1 and ROUGE-L metrics as well as the official ROUGE-1/2/L implementation provided by the HuggingFace `evaluate` library. In addition to quantitative evaluation, we conducted qualitative analysis by inspecting generated summaries and comparing them against reference summaries and baseline outputs. Finally, we demonstrated a realistic downstream application by designing an action-first ChatGPT prompting layer that consumes model-generated summaries and extracts decisions, action items, and open questions.\n",
        "\n",
        "### Strengths\n",
        "\n",
        "- End-to-end transformer pipeline specifically tailored to dialogue summarization.\n",
        "- Comprehensive evaluation combining quantitative metrics with qualitative error analysis.\n",
        "- Clear demonstration of abstractive summarization behavior and its trade-offs relative to extractive baselines.\n",
        "- Practical system-level integration through an action-oriented LLM prompt, illustrating downstream usage and error propagation.\n",
        "\n",
        "### Limitations\n",
        "\n",
        "- Training time, model size, and number of fine-tuning steps were constrained by available hardware resources.\n",
        "- ROUGE metrics, while widely adopted, primarily measure lexical overlap and do not fully capture semantic faithfulness or factual grounding.\n",
        "- The model occasionally exhibits semantic drift and hallucinations due to limited fine-tuning data and the absence of explicit grounding or copy mechanisms.\n",
        "- Hyperparameter tuning was kept minimal to maintain computational feasibility.\n",
        "\n",
        "### Future Work\n",
        "\n",
        "Future extensions of this work could explore fine-tuning larger and more specialized encoder–decoder architectures such as BART or T5 for improved summarization quality. Incorporating semantic evaluation metrics (e.g., BERTScore) alongside ROUGE could provide a more faithful assessment of model performance. Additionally, integrating human feedback or reinforcement learning–based fine-tuning approaches could help reduce hallucinations and improve grounding. Finally, deploying the model behind an API and integrating it into a real-time chat or productivity application would allow for real-world evaluation and iterative improvement.\n"
      ],
      "metadata": {
        "id": "iPQU85c9mkjs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Remarks\n",
        "\n",
        "This project focuses on building and evaluating a realistic dialogue summarization pipeline under limited computational resources. While the model does not achieve state-of-the-art performance, it highlights key challenges of abstractive summarization and demonstrates how such models can be integrated into larger LLM-based systems.\n"
      ],
      "metadata": {
        "id": "sBtrNNbHpdhf"
      }
    }
  ]
}