{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-AHjzquymP5"
   },
   "source": [
    "# Project 3: Large Language Models for Dialogue Summarization\n",
    "\n",
    "This notebook presents an end-to-end transformer-based approach to abstractive dialogue summarization using the SAMSum dataset. We explore model training, evaluation, and downstream applications.\n",
    "\n",
    "We focus on:\n",
    "- building and fine-tuning a **BERT–GPT-2 encoder–decoder** model for abstractive summarization,\n",
    "- comparing it against a simple **heuristic baseline**,\n",
    "- computing both **custom and official ROUGE metrics**,\n",
    "- performing **qualitative error analysis** with real examples,\n",
    "- designing a **ChatGPT-style action-first prompting layer** on top of the summaries.\n",
    "\n",
    "The notebook is structured as follows:\n",
    "\n",
    "1. Setup and Reproducibility  \n",
    "2. Dataset Loading and Exploratory Data Analysis (EDA)  \n",
    "3. Tokenization and Preprocessing  \n",
    "4. BERT–GPT-2 Encoder–Decoder Model  \n",
    "5. Training with Seq2SeqTrainer  \n",
    "6. Baseline vs. Model: ROUGE Evaluation and Qualitative Analysis  \n",
    "7. Official ROUGE with `evaluate`  \n",
    "8. Action-First ChatGPT Prompting Layer  \n",
    "9. Conclusions and Next Steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bWMOsGZY3x2c"
   },
   "outputs": [],
   "source": [
    "!pip install -q evaluate rouge-score nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1zIZu0244Mxl"
   },
   "outputs": [],
   "source": [
    "# Fix Colab environment issues (REQUIRED)\n",
    "!pip install -q --upgrade datasets transformers huggingface_hub evaluate rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7g4a4XJyyn3e",
    "outputId": "e8bac52f-583e-4b08-f4ba-058e96cb0dbb"
   },
   "outputs": [],
   "source": [
    "# 1. Setup and Reproducibility\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    EncoderDecoderModel,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "\n",
    "from evaluate import load as load_metric\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"\n",
    "    Set random seeds for Python, NumPy and PyTorch to improve reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxkjCt_nysT7"
   },
   "source": [
    "## 2. Dataset Loading and Quick EDA\n",
    "\n",
    "We use the **SAMSum** dataset, which contains short messenger-style dialogues and\n",
    "human-written abstractive summaries.\n",
    "\n",
    "In this section we:\n",
    "- load the dataset using the `datasets` library,\n",
    "- inspect a few raw examples,\n",
    "- compute quick statistics on dialogue and summary lengths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D2cwxgP1yzWa",
    "outputId": "7818d1e1-54b7-4e78-e46b-3c88365f4523"
   },
   "outputs": [],
   "source": [
    "# 2.1 Load the SAMSum dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Try the official SAMSum dataset first\n",
    "try:\n",
    "    samsum = load_dataset(\"samsum\")\n",
    "    print(\"Loaded 'samsum' dataset from the Hub.\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to load 'samsum' directly. Error:\", e)\n",
    "    print(\"Trying the mirror dataset 'knkarthick/samsum' instead...\")\n",
    "    samsum = load_dataset(\"knkarthick/samsum\")\n",
    "    print(\"Loaded 'knkarthick/samsum' dataset successfully.\")\n",
    "\n",
    "print(samsum)\n",
    "print(\"Train size:\", len(samsum[\"train\"]))\n",
    "print(\"Validation size:\", len(samsum[\"validation\"]))\n",
    "print(\"Test size:\", len(samsum[\"test\"]))\n",
    "\n",
    "\n",
    "print(\"Train size:\", len(samsum[\"train\"]))\n",
    "print(\"Validation size:\", len(samsum[\"validation\"]))\n",
    "print(\"Test size:\", len(samsum[\"test\"]))\n",
    "\n",
    "\n",
    "# 2.2 Inspect a few sample dialogues\n",
    "for i in range(2):\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Dialogue:\\n\", samsum[\"train\"][i][\"dialogue\"])\n",
    "    print(\"\\nSummary:\\n\", samsum[\"train\"][i][\"summary\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P34_6U63zOv0"
   },
   "source": [
    "### 2.3 Quick Length Statistics\n",
    "\n",
    "We compute basic statistics (min, max, mean) for character lengths of dialogues and summaries\n",
    "on a subset of the training data. This helps us choose reasonable maximum sequence lengths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ghByApVI1bAO",
    "outputId": "51133f84-945b-471b-a290-71e37fbcae97"
   },
   "outputs": [],
   "source": [
    "def quick_stats(dataset, n_samples: int = 2000):\n",
    "    \"\"\"\n",
    "    Compute simple statistics (min, max, mean character length)\n",
    "    for dialogues and summaries in a dataset split.\n",
    "    \"\"\"\n",
    "    n_samples = min(n_samples, len(dataset))\n",
    "    dlg_lengths = []\n",
    "    sum_lengths = []\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        dlg = dataset[i][\"dialogue\"]\n",
    "        summ = dataset[i][\"summary\"]\n",
    "        dlg_lengths.append(len(dlg))\n",
    "        sum_lengths.append(len(summ))\n",
    "\n",
    "    stats = {\n",
    "        \"dialogue_min_len\": int(np.min(dlg_lengths)),\n",
    "        \"dialogue_max_len\": int(np.max(dlg_lengths)),\n",
    "        \"dialogue_mean_len\": float(np.mean(dlg_lengths)),\n",
    "        \"summary_min_len\": int(np.min(sum_lengths)),\n",
    "        \"summary_max_len\": int(np.max(sum_lengths)),\n",
    "        \"summary_mean_len\": float(np.mean(sum_lengths)),\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "\n",
    "train_stats = quick_stats(samsum[\"train\"], n_samples=2000)\n",
    "train_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmZFl6VP2Q8V"
   },
   "source": [
    "## 3. Tokenization and Preprocessing\n",
    "\n",
    "Our summarization model uses:\n",
    "\n",
    "- **BERT** as the encoder (for the input dialogue),\n",
    "- **GPT-2** as the decoder (for the target summary).\n",
    "\n",
    "We therefore use two tokenizers:\n",
    "\n",
    "- `bert-base-uncased` for dialogues,\n",
    "- `gpt2` for summaries.\n",
    "\n",
    "Preprocessing steps:\n",
    "- truncate/pad dialogues and summaries to fixed maximum lengths,\n",
    "- tokenize with the corresponding tokenizers,\n",
    "- replace padding tokens in labels with `-100` so that they are ignored by the loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_uwqnpRi2X9T",
    "outputId": "a956d0f5-80c9-453a-b65f-c747a59dbdf6"
   },
   "outputs": [],
   "source": [
    "# 3.1 Load tokenizers for encoder and decoder\n",
    "\n",
    "encoder_name = \"bert-base-uncased\"\n",
    "decoder_name = \"gpt2\"\n",
    "\n",
    "enc_tok = AutoTokenizer.from_pretrained(encoder_name, use_fast=True)\n",
    "dec_tok = AutoTokenizer.from_pretrained(decoder_name, use_fast=True)\n",
    "\n",
    "# GPT-2 does not have a PAD token by default, so we add one\n",
    "if dec_tok.pad_token is None:\n",
    "    dec_tok.add_special_tokens({\"pad_token\": dec_tok.eos_token})\n",
    "\n",
    "print(\"Encoder vocab size:\", enc_tok.vocab_size)\n",
    "print(\"Decoder vocab size:\", len(dec_tok))\n",
    "print(\"Decoder PAD token:\", dec_tok.pad_token, dec_tok.pad_token_id)\n",
    "\n",
    "# 3.2 Define maximum sequence lengths based on data stats\n",
    "MAX_INPUT_LEN = 256   # maximum tokens for dialogues\n",
    "MAX_TARGET_LEN = 64   # maximum tokens for summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2sq6pnY5C12A",
    "outputId": "b8710048-34a1-409e-ca03-f6bdf240cb5e"
   },
   "outputs": [],
   "source": [
    "# 3.3 Preprocessing Function\n",
    "\n",
    "def preprocess_batch(batch):\n",
    "    \"\"\"\n",
    "    Tokenize dialogues and summaries for encoder-decoder training.\n",
    "\n",
    "    - Encoder: BERT tokenizer on 'dialogue'\n",
    "    - Decoder: GPT-2 tokenizer on 'summary'\n",
    "    - PAD tokens in labels are replaced with -100 so they are ignored by the loss.\n",
    "\n",
    "    Args:\n",
    "        batch: A batch of examples with 'dialogue' and 'summary' fields.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with input_ids, attention_mask, and labels.\n",
    "    \"\"\"\n",
    "    # Tokenize dialogues for the encoder\n",
    "    enc = enc_tok(\n",
    "        batch[\"dialogue\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_INPUT_LEN,\n",
    "    )\n",
    "\n",
    "    # Tokenize summaries for the decoder\n",
    "    dec = dec_tok(\n",
    "        batch[\"summary\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_TARGET_LEN,\n",
    "    )\n",
    "\n",
    "    # Use decoder token IDs as labels\n",
    "    enc[\"labels\"] = dec[\"input_ids\"]\n",
    "\n",
    "    # Replace padding token IDs in labels with -100 so they are ignored by the loss\n",
    "    enc[\"labels\"] = [\n",
    "        [(t if t != dec_tok.pad_token_id else -100) for t in seq]\n",
    "        for seq in enc[\"labels\"]\n",
    "    ]\n",
    "\n",
    "    return enc\n",
    "\n",
    "\n",
    "# 3.4 (Optional) Subsample dataset for faster training (Colab-friendly)\n",
    "\n",
    "TRAIN_N = 6000\n",
    "VAL_N = 1500\n",
    "\n",
    "train_raw = samsum[\"train\"].select(range(min(TRAIN_N, len(samsum[\"train\"]))))\n",
    "val_raw = samsum[\"validation\"].select(range(min(VAL_N, len(samsum[\"validation\"]))))\n",
    "\n",
    "# 3.5 Apply preprocessing\n",
    "\n",
    "train_tokenized = train_raw.map(\n",
    "    preprocess_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[\"dialogue\", \"summary\"],\n",
    ")\n",
    "\n",
    "val_tokenized = val_raw.map(\n",
    "    preprocess_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[\"dialogue\", \"summary\"],\n",
    ")\n",
    "\n",
    "train_tokenized[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2i_9aJ5U2k7e"
   },
   "source": [
    "## 4. BERT–GPT-2 Encoder–Decoder Model\n",
    "\n",
    "We build a **BERT–GPT-2 encoder–decoder model** initialized from pretrained weights:\n",
    "\n",
    "- the encoder is `bert-base-uncased`,\n",
    "- the decoder is `gpt2`,\n",
    "- cross-attention is enabled in the decoder by default,\n",
    "- we configure the correct special tokens (PAD, EOS, decoder start).\n",
    "\n",
    "This setup leverages strong contextual representations from BERT\n",
    "and fluent generative capabilities from GPT-2, making it suitable for abstractive dialogue summarization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591,
     "referenced_widgets": [
      "6ec4c1a7b76a435baccdcef020471909",
      "1d96c01e96a0485ab727785a34ecef3e",
      "2be75450379a47a68621f46905f983c9",
      "d1ba88b6db594e6988447dc8386afca8",
      "212096645c26474ead386325dcabe667",
      "93bfbe99943b40d2bf969a7ea8dee8fd",
      "7f6fa51e9f2144c7b577f7ab0c898389",
      "77b4b95f4bbe49f3ad023f3f5776f322",
      "0426b9ec92ee41119c40d407659630fe",
      "797432878eb64d7781ad13108ab2ac49",
      "f6cbfa3a4a0b43779fb4a8fb84a276c6",
      "db01592f9e824fde83b6a2d09dbe9f20",
      "0ae0201e64cb4ac79d69bed66151d1cd",
      "368a7203475b4886ab88a2dedc99c6d4",
      "1701df8d595747f38f8a2fbb7044d22f",
      "ca1a9e2028a546df8aba6fd8a3e34ca2",
      "c54b6ba7279449f9977840f0e3d001cb",
      "2251fc8d1ebe4c6fb1c8508aa01aafea",
      "610a224046e24048a78e763f46f76e37",
      "bf247859160a4ad591325024923a7e52",
      "a861e68ea15e45068bf81ae86e6e8380",
      "70aa2f46961a4a5e94e085f16a748f49"
     ]
    },
    "id": "XXmWrl6k2mM9",
    "outputId": "26255018-b8d8-4dff-f785-feaa65e95127"
   },
   "outputs": [],
   "source": [
    "# 4.1 Build encoder–decoder model from pretrained components\n",
    "\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    encoder_name,\n",
    "    decoder_name,\n",
    ")\n",
    "\n",
    "# If we added a PAD token to the decoder tokenizer, we need to resize embeddings\n",
    "model.decoder.resize_token_embeddings(len(dec_tok))\n",
    "\n",
    "# 4.2 Configure special tokens and generation settings\n",
    "# Special tokens (это правильно оставляем в model.config)\n",
    "model.config.decoder_start_token_id = dec_tok.pad_token_id\n",
    "model.config.eos_token_id = dec_tok.eos_token_id\n",
    "model.config.pad_token_id = dec_tok.pad_token_id\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# >>> Generation parameters должны жить в model.generation_config <<<\n",
    "gen_cfg = model.generation_config\n",
    "gen_cfg.max_length = MAX_TARGET_LEN\n",
    "gen_cfg.min_length = 8\n",
    "gen_cfg.no_repeat_ngram_size = 2\n",
    "#gen_cfg.early_stopping = True\n",
    "model.generation_config = gen_cfg\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "print(\"Model initialized and moved to device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EfXP2GwE2qtR"
   },
   "source": [
    "## 5. Data Collator and Training Setup\n",
    "\n",
    "We use the Hugging Face **Seq2SeqTrainer** API:\n",
    "\n",
    "- `DataCollatorForSeq2Seq` dynamically pads examples to the longest sequence in the batch,\n",
    "- `Seq2SeqTrainingArguments` define hyperparameters,\n",
    "- `Seq2SeqTrainer` handles training, evaluation, and generation.\n",
    "\n",
    "We keep training settings moderate to fit within typical Google Colab limits,\n",
    "but they can be scaled up for more serious experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JayA7Inx2tqn"
   },
   "outputs": [],
   "source": [
    "# 5.1 Data collator for seq2seq models\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=enc_tok,\n",
    "    model=model,\n",
    "    padding=\"longest\",\n",
    ")\n",
    "\n",
    "\n",
    "# 5.2 Seq2Seq training arguments (version-safe)\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"outputs_samsum_bert2gpt2\",\n",
    "    num_train_epochs=1,\n",
    "    max_steps=250,                      # keep small for Colab; increase if you can\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,                         # set True if your GPU supports it\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    logging_steps=50,                   # how often to log\n",
    "    save_steps=1000,                    # effectively no checkpoints\n",
    "    save_total_limit=1,\n",
    "    report_to=[],                       # no external logging (W&B etc.)\n",
    ")\n",
    "\n",
    "# 5.3 Initialize the Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=val_tokenized,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VeE8zds-2v4j"
   },
   "source": [
    "## 6. Training the Encoder–Decoder Model\n",
    "\n",
    "We now fine-tune the pretrained BERT–GPT-2 encoder–decoder model on SAMSum.\n",
    "\n",
    "The `Seq2SeqTrainer`:\n",
    "- performs forward and backward passes,\n",
    "- handles optimizer and learning rate scheduling,\n",
    "- periodically evaluates on the validation set.\n",
    "\n",
    "For a stronger model, we would train for more steps and possibly with larger batch sizes.\n",
    "Here we focus on a **Colab-friendly but realistic** training run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472,
     "referenced_widgets": [
      "b014405bc37b44acb146f4110ae04ae1",
      "dc85f6c3805e479f8b43e69a9b860d9e",
      "ce2e333a3a61403d8f2803b18a2c1d05",
      "cd58cffd93044e91a5f55aa14600fca0",
      "8d116d7e77c24a22865c064d942c8668",
      "12283ef649ff414eb032d69e955cc9b5",
      "d6a78370e0fe460584bee54c7c7eb6a9",
      "b844a767f9044bceb4e9a21d71f92deb",
      "078a95f4d8524bc8893ecfe14d5b0719",
      "2e784b12cc064252a77a81a2adba8bd2",
      "a8559cd32bd545bcae8da3a13141f0f1"
     ]
    },
    "id": "9IOYPxXE2yvn",
    "outputId": "df99d033-8fef-49be-a0e9-0f7da6c63c65"
   },
   "outputs": [],
   "source": [
    "# 6.1 Train the model (this may take several minutes depending on hardware)\n",
    "\n",
    "train_result = trainer.train()\n",
    "print(\"Training finished. Global step:\", train_result.global_step)\n",
    "\n",
    "# 6.2 Evaluate the model using Trainer's built-in evaluation\n",
    "eval_metrics = trainer.evaluate()\n",
    "print(\"Validation metrics from Trainer:\", eval_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7uGEdGoB20nZ"
   },
   "source": [
    "## 7. Baseline and Custom ROUGE Implementation\n",
    "\n",
    "To better interpret model performance, we define:\n",
    "\n",
    "1. A very simple **baseline summarizer**:\n",
    "   - It takes the first and last sentence of the dialogue as the \"summary\".\n",
    "2. A custom implementation of **ROUGE-1** and **ROUGE-L** F1:\n",
    "   - This demonstrates understanding of how ROUGE works,\n",
    "   - and allows us to compare baseline vs. model on the same metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6ejIfFW22VS"
   },
   "outputs": [],
   "source": [
    "def split_into_sentences(text: str):\n",
    "    \"\"\"\n",
    "    Very simple sentence splitter based on punctuation.\n",
    "    This is intentionally lightweight and heuristic.\n",
    "    \"\"\"\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    sentences = [s for s in sentences if s]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def baseline_head_tail(dialogue: str, max_sentences: int = 2) -> str:\n",
    "    \"\"\"\n",
    "    Baseline summarizer: take the first and last sentence from the dialogue.\n",
    "    If the dialogue is very short, return what is available.\n",
    "\n",
    "    This provides a naive extractive baseline to compare against our model.\n",
    "    \"\"\"\n",
    "    sentences = split_into_sentences(dialogue)\n",
    "    if not sentences:\n",
    "        return \"\"\n",
    "    if len(sentences) <= max_sentences:\n",
    "        return \" \".join(sentences)\n",
    "    # use the first and last sentence\n",
    "    return sentences[0] + \" \" + sentences[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWQSdRz2253F"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def _tok(s: str):\n",
    "    \"\"\"\n",
    "    Very simple whitespace-based tokenizer with lowercasing.\n",
    "    \"\"\"\n",
    "    return s.lower().split()\n",
    "\n",
    "\n",
    "def rouge1_f1_single(pred: str, ref: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute ROUGE-1 F1 score between a single prediction and reference.\n",
    "    \"\"\"\n",
    "    pred_tokens = _tok(pred)\n",
    "    ref_tokens = _tok(ref)\n",
    "    if not pred_tokens or not ref_tokens:\n",
    "        return 0.0\n",
    "\n",
    "    pred_counts = Counter(pred_tokens)\n",
    "    ref_counts = Counter(ref_tokens)\n",
    "\n",
    "    overlap = sum((pred_counts & ref_counts).values())\n",
    "    if overlap == 0:\n",
    "        return 0.0\n",
    "\n",
    "    precision = overlap / len(pred_tokens)\n",
    "    recall = overlap / len(ref_tokens)\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "\n",
    "def lcs_len(x: list, y: list) -> int:\n",
    "    \"\"\"\n",
    "    Longest Common Subsequence (LCS) length for ROUGE-L.\n",
    "    \"\"\"\n",
    "    m, n = len(x), len(y)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            if x[i] == y[j]:\n",
    "                dp[i + 1][j + 1] = dp[i][j] + 1\n",
    "            else:\n",
    "                dp[i + 1][j + 1] = max(dp[i][j + 1], dp[i + 1][j])\n",
    "    return dp[m][n]\n",
    "\n",
    "\n",
    "def rougeL_f1_single(pred: str, ref: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute ROUGE-L F1 score between a single prediction and reference.\n",
    "    \"\"\"\n",
    "    pred_tokens = _tok(pred)\n",
    "    ref_tokens = _tok(ref)\n",
    "    if not pred_tokens or not ref_tokens:\n",
    "        return 0.0\n",
    "\n",
    "    lcs = lcs_len(pred_tokens, ref_tokens)\n",
    "    if lcs == 0:\n",
    "        return 0.0\n",
    "\n",
    "    precision = lcs / len(pred_tokens)\n",
    "    recall = lcs / len(ref_tokens)\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "\n",
    "def avg_scores(pred_list, ref_list, metric_fn):\n",
    "    \"\"\"\n",
    "    Compute the average of a given ROUGE metric over a list of predictions.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for p, r in zip(pred_list, ref_list):\n",
    "        scores.append(metric_fn(p, r))\n",
    "    return float(np.mean(scores)), scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3LD8CpK28Ee"
   },
   "source": [
    "## 8. Generating Model Summaries & Comparing with Baseline\n",
    "\n",
    "In this section we:\n",
    "\n",
    "1. Build a small evaluation subset from the validation split.\n",
    "2. Generate summaries using:\n",
    "   - the **baseline** heuristic,\n",
    "   - our fine-tuned **encoder–decoder model**.\n",
    "3. Compute **custom ROUGE-1 and ROUGE-L** scores for both.\n",
    "4. Inspect a few **qualitative examples** to understand typical model behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qrZTcbpx2-Ol"
   },
   "outputs": [],
   "source": [
    "def generate_summary(dialogue: str, max_new_tokens: int = 48) -> str:\n",
    "    \"\"\"\n",
    "    Generate an abstractive summary for a single dialogue using the\n",
    "    encoder–decoder model.\n",
    "\n",
    "    Args:\n",
    "        dialogue: Original SAMSum-style conversation (multi-turn).\n",
    "        max_new_tokens: Maximum number of tokens to generate.\n",
    "\n",
    "    Returns:\n",
    "        Decoded summary string.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    inputs = enc_tok(\n",
    "        [dialogue],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_INPUT_LEN,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_beams=4,\n",
    "            early_stopping=True,\n",
    "        )\n",
    "\n",
    "    summary = dec_tok.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return summary.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SS452oZK3AD-",
    "outputId": "adf7bb60-3007-4b29-8eac-88df04c11f88"
   },
   "outputs": [],
   "source": [
    "# 8.2 Build a small evaluation subset for qualitative and quantitative analysis\n",
    "EVAL_N = 150\n",
    "eval_raw = val_raw.select(range(min(EVAL_N, len(val_raw))))\n",
    "\n",
    "dialogues = [ex[\"dialogue\"] for ex in eval_raw]\n",
    "references = [ex[\"summary\"] for ex in eval_raw]\n",
    "\n",
    "# Baseline predictions\n",
    "baseline_preds = [baseline_head_tail(dlg) for dlg in dialogues]\n",
    "\n",
    "# Model predictions\n",
    "model_preds = [generate_summary(dlg) for dlg in dialogues]\n",
    "\n",
    "print(\"Example model prediction:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Dialogue:\\n\", dialogues[0])\n",
    "print(\"\\nReference summary:\\n\", references[0])\n",
    "print(\"\\nBaseline summary:\\n\", baseline_preds[0])\n",
    "print(\"\\nModel summary:\\n\", model_preds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "niL2dPoe3Bz7",
    "outputId": "4405d27f-2e99-4a4d-c95d-69a960f3cb35"
   },
   "outputs": [],
   "source": [
    "# 8.3 Compute custom ROUGE-1 and ROUGE-L for baseline and model\n",
    "\n",
    "base_r1_mean, base_r1_list = avg_scores(baseline_preds, references, rouge1_f1_single)\n",
    "base_rL_mean, base_rL_list = avg_scores(baseline_preds, references, rougeL_f1_single)\n",
    "\n",
    "mod_r1_mean, mod_r1_list = avg_scores(model_preds, references, rouge1_f1_single)\n",
    "mod_rL_mean, mod_rL_list = avg_scores(model_preds, references, rougeL_f1_single)\n",
    "\n",
    "print(\"=== Custom ROUGE (approximate) ===\")\n",
    "print(f\"Baseline ROUGE-1 F1: {base_r1_mean:.4f}\")\n",
    "print(f\"Baseline ROUGE-L F1: {base_rL_mean:.4f}\")\n",
    "print(f\"Model    ROUGE-1 F1: {mod_r1_mean:.4f}\")\n",
    "print(f\"Model    ROUGE-L F1: {mod_rL_mean:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xvr6egke3E-0",
    "outputId": "cc6c5b1e-2b6c-4136-a57b-27751fea5ace"
   },
   "outputs": [],
   "source": [
    "# 8.4 Show a few qualitative examples with scores\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Example {i+1}\")\n",
    "    print(\"--- Dialogue ---\")\n",
    "    print(dialogues[i])\n",
    "    print(\"\\n--- Reference summary ---\")\n",
    "    print(references[i])\n",
    "    print(\"\\n--- Baseline summary ---\")\n",
    "    print(baseline_preds[i])\n",
    "    print(\"\\n--- Model summary ---\")\n",
    "    print(model_preds[i])\n",
    "    print(\n",
    "        f\"\\nBaseline ROUGE-1 F1: {base_r1_list[i]:.4f} | ROUGE-L F1: {base_rL_list[i]:.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Model    ROUGE-1 F1: {mod_r1_list[i]:.4f} | ROUGE-L F1: {mod_rL_list[i]:.4f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFU7P9C53Gq4"
   },
   "source": [
    "## 9. Official ROUGE via `evaluate`\n",
    "\n",
    "In addition to the custom implementation, we compute **standard ROUGE metrics**\n",
    "using the `evaluate` library. This provides:\n",
    "\n",
    "- ROUGE-1, ROUGE-2, and ROUGE-L,\n",
    "- precision, recall, and F1 scores.\n",
    "\n",
    "We report these for our encoder–decoder model on the evaluation subset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n8t2VpRK3JC-",
    "outputId": "325bb1be-9dfc-4af9-85b6-2aacdd63c9d2"
   },
   "outputs": [],
   "source": [
    "from evaluate import load as load_metric\n",
    "\n",
    "rouge_metric = load_metric(\"rouge\")\n",
    "\n",
    "rouge_result = rouge_metric.compute(\n",
    "    predictions=model_preds,\n",
    "    references=references,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print(\"=== HuggingFace evaluate ROUGE (model) ===\")\n",
    "for key in [\"rouge1\", \"rouge2\", \"rougeL\"]:\n",
    "    score = rouge_result[key]\n",
    "    score_float = float(score)\n",
    "    print(f\"{key.upper()} F1: {score_float:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMVQ9x1t3LHe"
   },
   "source": [
    "## 10. Action-First ChatGPT Prompting Layer\n",
    "\n",
    "Beyond pure summarization, we add an **action-first prompting layer** designed for ChatGPT\n",
    "(or any comparable auto-regressive LLM).\n",
    "\n",
    "Workflow:\n",
    "\n",
    "1. Our encoder–decoder model produces a concise summary of a conversation.\n",
    "2. We wrap this summary into a structured prompt that asks the LLM to:\n",
    "   - extract key decisions,\n",
    "   - list action items with owners and deadlines,\n",
    "   - identify open questions or unresolved issues.\n",
    "\n",
    "This demonstrates how a custom summarization model can be integrated into a larger LLM-powered assistant for productivity and decision support.\n",
    "\n",
    "This experiment further highlights how summarization errors can propagate into downstream LLM-based systems, emphasizing the importance of faithful and grounded summaries in real-world applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "orJAhtAl3Mvs",
    "outputId": "1ab0d95f-c3bd-4314-f8b8-ff9fadf4a176"
   },
   "outputs": [],
   "source": [
    "ACTION_DIGEST_PROMPT = \"\"\"\n",
    "You are an AI assistant that helps busy professionals keep track of decisions and next steps.\n",
    "\n",
    "Below is a summary of a conversation. Your job is:\n",
    "1. Extract the key decisions that were made.\n",
    "2. Extract the action items with clear owners and deadlines if mentioned.\n",
    "3. Highlight any open questions or unresolved issues.\n",
    "\n",
    "Return your output in the following structured format:\n",
    "\n",
    "Decisions:\n",
    "- ...\n",
    "\n",
    "Action Items:\n",
    "- [Owner] ... (deadline: ...)\n",
    "\n",
    "Open Questions:\n",
    "- ...\n",
    "\n",
    "Conversation summary:\n",
    "\\\"\\\"\\\"{summary}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def build_action_digest_prompt(summary: str) -> str:\n",
    "    \"\"\"\n",
    "    Wrap a model-generated summary into an 'action-first' prompt that can be\n",
    "    sent to ChatGPT (or another LLM) to extract decisions and next steps.\n",
    "    \"\"\"\n",
    "    return ACTION_DIGEST_PROMPT.format(summary=summary)\n",
    "\n",
    "\n",
    "# 10.2 Example: use a model-generated summary and build the LLM prompt\n",
    "example_idx = 0\n",
    "example_summary = model_preds[example_idx]\n",
    "example_prompt = build_action_digest_prompt(example_summary)\n",
    "\n",
    "print(\"=== Example action-first prompt for ChatGPT ===\")\n",
    "print(example_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPQU85c9mkjs"
   },
   "source": [
    "## 11. Conclusions and Next Steps\n",
    "\n",
    "In this project, we developed and evaluated an end-to-end transformer-based system for abstractive dialogue summarization. We implemented a BERT–GPT-2 encoder–decoder architecture and fine-tuned it on the SAMSum dataset using a Colab-compatible training setup. To establish a meaningful point of comparison, we defined a simple extractive baseline that concatenates the first and last utterances of each dialogue.\n",
    "\n",
    "Model performance was evaluated using both custom-implemented ROUGE-1 and ROUGE-L metrics as well as the official ROUGE-1/2/L implementation provided by the HuggingFace `evaluate` library. In addition to quantitative evaluation, we conducted qualitative analysis by inspecting generated summaries and comparing them against reference summaries and baseline outputs. Finally, we demonstrated a realistic downstream application by designing an action-first ChatGPT prompting layer that consumes model-generated summaries and extracts decisions, action items, and open questions.\n",
    "\n",
    "### Strengths\n",
    "\n",
    "- End-to-end transformer pipeline specifically tailored to dialogue summarization.\n",
    "- Comprehensive evaluation combining quantitative metrics with qualitative error analysis.\n",
    "- Clear demonstration of abstractive summarization behavior and its trade-offs relative to extractive baselines.\n",
    "- Practical system-level integration through an action-oriented LLM prompt, illustrating downstream usage and error propagation.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- Training time, model size, and number of fine-tuning steps were constrained by available hardware resources.\n",
    "- ROUGE metrics, while widely adopted, primarily measure lexical overlap and do not fully capture semantic faithfulness or factual grounding.\n",
    "- The model occasionally exhibits semantic drift and hallucinations due to limited fine-tuning data and the absence of explicit grounding or copy mechanisms.\n",
    "- Hyperparameter tuning was kept minimal to maintain computational feasibility.\n",
    "\n",
    "### Future Work\n",
    "\n",
    "Future extensions of this work could explore fine-tuning larger and more specialized encoder–decoder architectures such as BART or T5 for improved summarization quality. Incorporating semantic evaluation metrics (e.g., BERTScore) alongside ROUGE could provide a more faithful assessment of model performance. Additionally, integrating human feedback or reinforcement learning–based fine-tuning approaches could help reduce hallucinations and improve grounding. Finally, deploying the model behind an API and integrating it into a real-time chat or productivity application would allow for real-world evaluation and iterative improvement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBtrNNbHpdhf"
   },
   "source": [
    "### Final Remarks\n",
    "\n",
    "This project focuses on building and evaluating a realistic dialogue summarization pipeline under limited computational resources. While the model does not achieve state-of-the-art performance, it highlights key challenges of abstractive summarization and demonstrates how such models can be integrated into larger LLM-based systems.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0426b9ec92ee41119c40d407659630fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "078a95f4d8524bc8893ecfe14d5b0719": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0ae0201e64cb4ac79d69bed66151d1cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c54b6ba7279449f9977840f0e3d001cb",
      "placeholder": "​",
      "style": "IPY_MODEL_2251fc8d1ebe4c6fb1c8508aa01aafea",
      "value": "Loading weights: 100%"
     }
    },
    "12283ef649ff414eb032d69e955cc9b5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1701df8d595747f38f8a2fbb7044d22f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a861e68ea15e45068bf81ae86e6e8380",
      "placeholder": "​",
      "style": "IPY_MODEL_70aa2f46961a4a5e94e085f16a748f49",
      "value": " 148/148 [00:00&lt;00:00, 322.34it/s, Materializing param=transformer.wte.weight]"
     }
    },
    "1d96c01e96a0485ab727785a34ecef3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93bfbe99943b40d2bf969a7ea8dee8fd",
      "placeholder": "​",
      "style": "IPY_MODEL_7f6fa51e9f2144c7b577f7ab0c898389",
      "value": "Loading weights: 100%"
     }
    },
    "212096645c26474ead386325dcabe667": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2251fc8d1ebe4c6fb1c8508aa01aafea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2be75450379a47a68621f46905f983c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_77b4b95f4bbe49f3ad023f3f5776f322",
      "max": 199,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0426b9ec92ee41119c40d407659630fe",
      "value": 199
     }
    },
    "2e784b12cc064252a77a81a2adba8bd2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "368a7203475b4886ab88a2dedc99c6d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_610a224046e24048a78e763f46f76e37",
      "max": 148,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bf247859160a4ad591325024923a7e52",
      "value": 148
     }
    },
    "610a224046e24048a78e763f46f76e37": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ec4c1a7b76a435baccdcef020471909": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1d96c01e96a0485ab727785a34ecef3e",
       "IPY_MODEL_2be75450379a47a68621f46905f983c9",
       "IPY_MODEL_d1ba88b6db594e6988447dc8386afca8"
      ],
      "layout": "IPY_MODEL_212096645c26474ead386325dcabe667"
     }
    },
    "70aa2f46961a4a5e94e085f16a748f49": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "77b4b95f4bbe49f3ad023f3f5776f322": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "797432878eb64d7781ad13108ab2ac49": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f6fa51e9f2144c7b577f7ab0c898389": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8d116d7e77c24a22865c064d942c8668": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "93bfbe99943b40d2bf969a7ea8dee8fd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a8559cd32bd545bcae8da3a13141f0f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a861e68ea15e45068bf81ae86e6e8380": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b014405bc37b44acb146f4110ae04ae1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dc85f6c3805e479f8b43e69a9b860d9e",
       "IPY_MODEL_ce2e333a3a61403d8f2803b18a2c1d05",
       "IPY_MODEL_cd58cffd93044e91a5f55aa14600fca0"
      ],
      "layout": "IPY_MODEL_8d116d7e77c24a22865c064d942c8668"
     }
    },
    "b844a767f9044bceb4e9a21d71f92deb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf247859160a4ad591325024923a7e52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c54b6ba7279449f9977840f0e3d001cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ca1a9e2028a546df8aba6fd8a3e34ca2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cd58cffd93044e91a5f55aa14600fca0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2e784b12cc064252a77a81a2adba8bd2",
      "placeholder": "​",
      "style": "IPY_MODEL_a8559cd32bd545bcae8da3a13141f0f1",
      "value": " 1/1 [00:16&lt;00:00, 16.58s/it]"
     }
    },
    "ce2e333a3a61403d8f2803b18a2c1d05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b844a767f9044bceb4e9a21d71f92deb",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_078a95f4d8524bc8893ecfe14d5b0719",
      "value": 1
     }
    },
    "d1ba88b6db594e6988447dc8386afca8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_797432878eb64d7781ad13108ab2ac49",
      "placeholder": "​",
      "style": "IPY_MODEL_f6cbfa3a4a0b43779fb4a8fb84a276c6",
      "value": " 199/199 [00:00&lt;00:00, 325.99it/s, Materializing param=pooler.dense.weight]"
     }
    },
    "d6a78370e0fe460584bee54c7c7eb6a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "db01592f9e824fde83b6a2d09dbe9f20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0ae0201e64cb4ac79d69bed66151d1cd",
       "IPY_MODEL_368a7203475b4886ab88a2dedc99c6d4",
       "IPY_MODEL_1701df8d595747f38f8a2fbb7044d22f"
      ],
      "layout": "IPY_MODEL_ca1a9e2028a546df8aba6fd8a3e34ca2"
     }
    },
    "dc85f6c3805e479f8b43e69a9b860d9e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_12283ef649ff414eb032d69e955cc9b5",
      "placeholder": "​",
      "style": "IPY_MODEL_d6a78370e0fe460584bee54c7c7eb6a9",
      "value": "Writing model shards: 100%"
     }
    },
    "f6cbfa3a4a0b43779fb4a8fb84a276c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
